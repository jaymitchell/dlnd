{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Siraj's Live Stream\n",
    "\n",
    "https://www.youtube.com/watch?v=p69khggr1Jo\n",
    "\n",
    "Awesome [introduction to neural networks](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)\n",
    "\n",
    "1. A Neural Network is an algorithm that learns to identify patterns in data\n",
    "2. Backpropagation is a technique to train a Neural Network by updating weights via gradient descent\n",
    "3. Deep learning = many layer neural net + big data + big compute\n",
    "\n",
    "[Hello TensorFlow!](https://www.oreilly.com/learning/hello-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The [difference between linear regression and logistic regression](http://stackoverflow.com/questions/12146914/what-is-the-difference-between-linear-regression-and-logistic-regression).... Linear regression finds the line of best fit for all items in a continuous set of data. Logistic regression find a line that separates discrete categories of data. E.g., suppose we have data that represents the features of houses and the price the houses sold for. Linear regression would attempt to draw a line that best fits the data, so you can take a new set of features and output an expected house price. On the other hand, logistic regression could draw a line that separates houses under \\$200k and houses equal to or greater than \\$200k.\n",
    "\n",
    "Luis used the logistic regression example of students that get accepted and rejected from a university based on test scores and grades.\n",
    "\n",
    "Both linear regression and logistic regression use gradient descent to determine the line.\n",
    "\n",
    "Rather than minimizing the number of errors we minimize the \"log loss function\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Networks\n",
    "\n",
    "Sometimes a line isn't enough to represent the correct classification. The example was that you need high grades and high test scores to get accepted to a university. In other words only those in the top right of the graph are accepted. In this case you need two lines (one for grades and one for tests) to isolate those in the top right of the graph. This is a neural network.\n",
    "\n",
    "1. Does the student have high grades (based on logistic regression)?\n",
    "2. Does the student have high test scores (based on logistic regression)?\n",
    "3. Are the answers to #1 and #2 both \"yes\" (AND operator)?\n",
    "\n",
    "Joining the inputs and those three nodes then we have a neural network that outputs whether or not a student should be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "Individual nodes are called perceptrons or neruons. It's up to the neural network to learn which features are most important - i.e., the **weight** given to an input.\n",
    "\n",
    "## Weights\n",
    "\n",
    "The network is **trained** based on prior data to determine the correct weight for each input. (Higher weight means it is more important.)\n",
    "\n",
    "When writing neural network equations weights will be represented by *w* - as in $W$ for a matrix of weights, or $w$ for an individual weight.\n",
    "\n",
    "If you have $w_{grades} = -1$ and $w_{test} = -0.2$, $w_{grades}$ has 5 times more impact on the neural network. I.e., the *relative* values are what matter.\n",
    "\n",
    "Perceptrons use **linear combination** to apply the weights to the inputs. I.e., $w_{grades}\\cdot x_{grades} + w_{test}\\cdot x_{test} = -0.2\\cdot x_{grades} -1\\cdot x_{test}$\n",
    "\n",
    "Replace $grades$ with $1$ and $tests$ with $2$ to get: $w_1\\cdot x_1 + w_2\\cdot x_2$\n",
    "\n",
    "For weights and inputs 1 to $m$, this becomes:\n",
    "\n",
    "$$\\sum_{i=1}^m w_i\\cdot x_i$$\n",
    "\n",
    "## Calculating the output with an activation function\n",
    "\n",
    "The results of the linear combination are turned into an output signal by sending them into an **activation function**.\n",
    "\n",
    "One example of an activation function is **Heaviside step function** which is simply:\n",
    "\n",
    "$$\n",
    "f(h) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } h\\lt 0\\\\\n",
    "1 & \\text{if } h\\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Adding a **bias**, represented as $b$, allows us to adjust the scores. As with weights, biases can be adjusted.\n",
    "\n",
    "With a bias and using the heaviside step function, the complete formula for a perceptron is:\n",
    "\n",
    "$$\n",
    "f(x_1, x_2, \\ldots, x_m) = \n",
    "\\begin{cases}\n",
    "0 & \\text{if } b + \\sum w_i \\cdot x_i \\lt 0\\\\\n",
    "1 & \\text{if } b + \\sum w_i \\cdot x_i \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The weights and bias are assigned a random value, and then updated with an algorithm like gradient descent. This is how the network learns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AND Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Confidence    Activation Output   Is Correct\n",
      "      0          0         -0.15                    0          Yes\n",
      "      0          1         -0.05                    0          Yes\n",
      "      1          0         -0.05                    0          Yes\n",
      "      1          1          0.05                    1          Yes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weight1 = 0.1\n",
    "weight2 = 0.1\n",
    "bias = -0.15\n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [False, False, False, True]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    confidence = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(confidence > 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], confidence, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Confidence', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NOT Perceptron\n",
    "\n",
    "The following code represent a perceptron that performs the NOT operation for the second input only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice!  You got it all correct.\n",
      "\n",
      "Input 1    Input 2    Confidence    Activation Output   Is Correct\n",
      "      0          0           1.0                    1          Yes\n",
      "      0          1          -1.0                    0          Yes\n",
      "      1          0           1.0                    1          Yes\n",
      "      1          1          -1.0                    0          Yes\n"
     ]
    }
   ],
   "source": [
    "weight1 = 0\n",
    "weight2 = -2.0\n",
    "bias = 1.0\n",
    "\n",
    "# Inputs and outputs\n",
    "test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "correct_outputs = [True, False, True, False]\n",
    "outputs = []\n",
    "\n",
    "# Generate and check output\n",
    "for test_input, correct_output in zip(test_inputs, correct_outputs):\n",
    "    confidence = weight1 * test_input[0] + weight2 * test_input[1] + bias\n",
    "    output = int(confidence > 0)\n",
    "    is_correct_string = 'Yes' if output == correct_output else 'No'\n",
    "    outputs.append([test_input[0], test_input[1], confidence, output, is_correct_string])\n",
    "\n",
    "# Print output\n",
    "num_wrong = len([output[4] for output in outputs if output[4] == 'No'])\n",
    "output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Confidence', '  Activation Output', '  Is Correct'])\n",
    "if not num_wrong:\n",
    "    print('Nice!  You got it all correct.\\n')\n",
    "else:\n",
    "    print('You got {} wrong.  Keep trying!\\n'.format(num_wrong))\n",
    "print(output_frame.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR Perceptron\n",
    "\n",
    "For more complicated logic, such as XOR, we may no longer be able to represent the logic in a single perceptron. Instead, we can use multiple perceptron layers to represent the logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "The activation function can be any function, $f(h)$, where the input, $h$, is: $h = \\sum_i w_ix_i + b$. For example, $f(h) = h$ is the same as linear regression, $y = \\sum_i w_ix_i + b$.\n",
    "\n",
    "The sigmoid, or logistic, activation function is:\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{(1 + e^{-x})}\n",
    "$$\n",
    "\n",
    "The output will be between 0 and 1, and can be viewed as the probability for success.\n",
    "\n",
    "Another nice thing about the sigmoid function is that its derivitive is easily calculated when we get to gradient descent. $f^\\prime = f(x) \\cdot (1 - f(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "0.432907095035\n"
     ]
    }
   ],
   "source": [
    "# A simple neural network (that doesn't learn)\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "inputs = np.array([0.7, -0.3])\n",
    "weights = np.array([0.1, 0.8])\n",
    "bias = -0.1\n",
    "\n",
    "output = sigmoid(np.dot(inputs, weights) + bias)\n",
    "\n",
    "print('Output:')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning weights\n",
    "\n",
    "Since the weights are the knobs we have to adjust for the output of the neural network, we want to learn the best values for the weights rather than setting them by hand. To learn which way to adjust the weights, the first thing we need to do is calculate how wrong our predictions were.\n",
    "\n",
    "Sum of squared errors (SSE) is a good choice for calculating this because the error is always positive and larger errors are penalized more than smaller errors.\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2} \\sum_{\\mu} \\sum_j \\left[ y_j^{\\mu} - \\hat y_j^{\\mu} \\right]^2\n",
    "$$\n",
    "\n",
    "$\\hat y_j^{\\mu} = f \\left( \\sum_i w_{ij}x_i^{\\mu} \\right)$ is the prediction of the network\n",
    "\n",
    "$y_j^{\\mu}$ is the actual value\n",
    "\n",
    "$y_j^{\\mu} - \\hat y_j^{\\mu}$ is the difference between the actual value\n",
    "\n",
    "$\\sum_j$ sums over all the ouput units $j$\n",
    "\n",
    "$\\sum_{\\mu}$ sums over all the data points $\\mu$\n",
    "\n",
    "$\\frac{1}{2}$ seems to be mainly to make the math easy when we take the derivative.\n",
    "\n",
    "To adjust the weights to get the errors as small as possible, use **gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Calculate the gradient of the squared error to find the steepest direction that minimizes the error the most. *Gradient* just means slope, which is just the function's derivative.\n",
    "\n",
    "There is some calculus to determine the formula for the gradient descent step in the lesson, but what it comes down to is:\n",
    "\n",
    "$$\n",
    "\\Delta w_{ij} = \\eta\\delta_jx_i\n",
    "$$\n",
    "\n",
    "Breaking that down (many of these are the same as the last section):\n",
    "\n",
    "$\\eta$ (eta) is the learning rate.\n",
    "\n",
    "$\\delta_j = \\left(y_j - \\hat y_j\\right)f^\\prime\\left(h_j\\right)$  represents the error gradient. ($\\delta$ is lowercase delta)\n",
    "\n",
    "$y_j$ is the actual value\n",
    "\n",
    "$\\hat y_j$ is the output unit $j$\n",
    "\n",
    "Therefore, $\\left(y_j - \\hat y_j\\right)$ is the prediction error\n",
    "\n",
    "$f^\\prime\\left(h_j\\right)$ is the gradient\n",
    "\n",
    "$h_j$ is the input to the output unit $j$ and is defined as $h_j = \\sum_i w_{ij}x_i$\n",
    "\n",
    "$w_{ij}$ is the weight\n",
    "\n",
    "$x_i$ is the input term\n",
    "\n",
    "Note that the larger any of those items are, the larger the step. E.g., if the prediction error is very large, we want to take a very large step. On the other hand, the closer we get to the correct answer, the smaller the steps become.\n",
    "\n",
    "Bias can be defined as $\\Delta b_j = \\eta \\delta_j$ (i.e., same as other weights, but just ignore the input).\n",
    "\n",
    "Note that gradient descent can be subject to local minima. There are ways to avoid that though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Maths Example\n",
    "\n",
    "Suppose we have the following:\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\begin{vmatrix}\n",
    "0.1 \\\\\n",
    "0.3\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "y = 0.2\n",
    "$$\n",
    "\n",
    "$$\n",
    "w =\n",
    "\\begin{vmatrix}\n",
    "-0.8 \\\\\n",
    "0.5\n",
    "\\end{vmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\eta = 0.5\n",
    "$$\n",
    "\n",
    "The neural network output using the sigmoid activation function would be:\n",
    "\n",
    "$$\n",
    "\\hat y = \\frac{1}{1 + e^{-((0.1\\cdot -0.8) + (0.3\\cdot 0.5))}} = \\frac{1}{1 + e^{-0.07}} = 0.51749\\dots\n",
    "$$\n",
    "\n",
    "$$\\left(y - \\hat y\\right) = -0.31749\\dots$$\n",
    "\n",
    "$$x \\cdot w = 0.06999$$\n",
    "\n",
    "$$\\delta = \\left(y - \\hat y\\right) f^{\\prime}\\left(w \\cdot w\\right) = -0.31749 \\cdot \\left(0.51749 \\cdot \\left(1 - 0.51749\\right)\\right) = -0.079275\\dots$$\n",
    "\n",
    "$$\\Delta w = \\eta\\delta x = 0.5 \\cdot -0.079275 \\cdot x =\n",
    "\\begin{vmatrix}\n",
    "-0.0039638\\dots \\\\\n",
    "-0.0118914\\dots\n",
    "\\end{vmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "0.377540668798\n",
      "Amount of Error:\n",
      "0.122459331202\n",
      "Change in Weights:\n",
      "[ 0.0143892  0.0287784]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "nn_output = sigmoid(np.dot(x, w))\n",
    "\n",
    "error = y - nn_output\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "change = learnrate * error * sigmoid_prime(np.dot(x, w))\n",
    "del_w = change * x\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Gradient Descent\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "### Categorical\n",
    "\n",
    "Transform categorical features into individual `0`/`1` features that represent the category. For example, if you ranked items between 1 and 4:\n",
    "\n",
    "| Rank |\n",
    "|------|\n",
    "| 1    |\n",
    "| 3    |\n",
    "| 4    |\n",
    "| 3    |\n",
    "| 2    |\n",
    "\n",
    "would become\n",
    "\n",
    "| Rank_1 | Rank_2 | Rank_3 | Rank_4 |\n",
    "|--------|--------|--------|--------|\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 0 | 0 | 1 |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "\n",
    "(Although Rank is a number, it is categorical data since it's about how items are grouped.)\n",
    "\n",
    "Pandas has `get_dummies` that does this. Their example:\n",
    "\n",
    "```python\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "```\n",
    "\n",
    "### Numerical\n",
    "\n",
    "The sigmoid function is very sensitive to really large or small inputs (Ng mentions any features with values of $\\pm 3$, I think) and squashes them. Therefore, normalize numerical data. For example, you could do z-score standardization or min-max scaling. The example does z-score standardization.\n",
    "\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html#about-min-max-scaling\n",
    "\n",
    "\n",
    "#### Z-score standardization\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Z-score standardization outputs data with $\\mu = 0$ (average) and $\\sigma = 1$ (standard deviation).\n",
    "\n",
    "They implemented it with:\n",
    "\n",
    "```python\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "```\n",
    "\n",
    "#### Min-Max scaling\n",
    "\n",
    "$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n",
    "\n",
    "This gives outputs a fixed range between 0 and 1.\n",
    "\n",
    "Min-max scaling can suppress the effects of outliers. Which to use depends on what your application needs - it seems like Z-score standardization is more most often the more appropriate choice.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Algorithm\n",
    "\n",
    "The general algorithm for updating the weights with gradient descent:\n",
    "\n",
    "- Set the weight step to zero: $\\Delta w_i = 0$ (note that this is $\\Delta w$, not $w$ which is explained below)\n",
    "- For each record in the training data:\n",
    "    - Make a forward pass through the network, calculating the output, $\\hat y = f(\\sum_i w_ix_i )$\n",
    "    - Calculate the error gradient in the output unit, $\\delta = (y - \\hat y) \\cdot f^{\\prime}(\\sum_i w_ix_i )$\n",
    "    - Update the weight step, $\\Delta w_i = \\Delta w_i + \\delta x_i$\n",
    "- Update the weights, $w_i = w_i + \\frac{\\eta \\Delta w_i}{m}$, where $m$ is the number of records.\n",
    "- Repeat for $e$ epochs.\n",
    "\n",
    "### Initializing weights\n",
    "\n",
    "It's important to initialize the weights randomly so they diverge. This is a good way to do it:\n",
    "\n",
    "```python\n",
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)\n",
    "```\n",
    "\n",
    "### Calculating h\n",
    "\n",
    "Reminder, $h$ is the input to the output unit:\n",
    "\n",
    "$$\n",
    "h = \\sum_i w_ix_i\n",
    "$$\n",
    "\n",
    "This is just the dot product of two arrays:\n",
    "\n",
    "```python\n",
    "output_in = np.dot(weights, inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gre</th>\n",
       "      <th>gpa</th>\n",
       "      <th>rank_1</th>\n",
       "      <th>rank_2</th>\n",
       "      <th>rank_3</th>\n",
       "      <th>rank_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.174867</td>\n",
       "      <td>-0.020759</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.030408</td>\n",
       "      <td>1.107943</td>\n",
       "      <td>0.303822</td>\n",
       "      <td>0.505736</td>\n",
       "      <td>0.464095</td>\n",
       "      <td>0.334932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.490553</td>\n",
       "      <td>-2.548567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.802483</td>\n",
       "      <td>-0.682929</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.019911</td>\n",
       "      <td>-0.091705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.452749</td>\n",
       "      <td>0.683454</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.837832</td>\n",
       "      <td>1.603135</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             gre        gpa     rank_1     rank_2     rank_3     rank_4\n",
       "count  40.000000  40.000000  40.000000  40.000000  40.000000  40.000000\n",
       "mean   -0.174867  -0.020759   0.100000   0.475000   0.300000   0.125000\n",
       "std     1.030408   1.107943   0.303822   0.505736   0.464095   0.334932\n",
       "min    -2.490553  -2.548567   0.000000   0.000000   0.000000   0.000000\n",
       "25%    -0.802483  -0.682929   0.000000   0.000000   0.000000   0.000000\n",
       "50%     0.019911  -0.091705   0.000000   0.000000   0.000000   0.000000\n",
       "75%     0.452749   0.683454   0.000000   1.000000   1.000000   0.000000\n",
       "max     1.837832   1.603135   1.000000   1.000000   1.000000   1.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is all Udacity code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "admissions = pd.read_csv('admissions.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standarize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "    \n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.ix[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "features_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.256060280051\n",
      "Train loss:  0.199324509463\n",
      "Train loss:  0.197636657787\n",
      "Train loss:  0.197270122133\n",
      "Train loss:  0.197152222857\n",
      "Train loss:  0.19710738054\n",
      "Train loss:  0.197089215266\n",
      "Train loss:  0.197081981947\n",
      "Train loss:  0.197079469107\n",
      "Train loss:  0.197079000687\n",
      "Prediction accuracy: 0.725\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Neural Network hyperparameters\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        # Loop through all records, x is the input, y is the target\n",
    "        output = sigmoid(np.dot(weights, x))\n",
    "        error = y - output\n",
    "        gradient = output / (1 - output)\n",
    "        del_w += error * gradient * x\n",
    "    weights += (learnrate * del_w) / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    "\n",
    "With a single output node, weights could be represented as an array. With multiple input and hidden layers (which is what makes deep learning so powerful), we need to represent weights as a matrix.\n",
    "\n",
    "$$\n",
    "\\begin{matrix}\n",
    "w_{11} & w_{12} \\\\\n",
    "w_{21} & w_{22} \\\\\n",
    "w_{31} & w_{32}\n",
    "\\end{matrix}\n",
    "$$\n",
    "\n",
    "The rows represent the input units and the columns represent the hidden units. I.e., $w_{32}$ is the weight for input unit 3 and hidden unit 2.\n",
    "\n",
    "To initialize the weights:\n",
    "\n",
    "```python\n",
    "# Number of records and input units\n",
    "n_records, n_inputs = features.shape\n",
    "# Number of hidden units\n",
    "n_hidden = 2\n",
    "weights = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "```\n",
    "\n",
    "This creates weights as an $n_{inputs} \\times n_{hidden}$ matrix.\n",
    "\n",
    "The inputs to the hidden layer $h_j$:\n",
    "\n",
    "$$\n",
    "h_j = \\sum_i w_{ij}x_i\n",
    "$$\n",
    "\n",
    "So for, the row vector of inputs $[x_1 x_2 x_3]$ the input for the first hidden unit would be the dot product of the input row vector and the first column of the weights matrix.\n",
    "\n",
    "$$\n",
    "h_1 = [x_1 x_2 x_2] \\times\n",
    "\\begin{matrix}\n",
    "w_{11} \\\\\n",
    "w_{21} \\\\\n",
    "w_{31}\n",
    "\\end{matrix}\n",
    "= x_1w_{11} + x_2w_{21} + x_3w_{31}\n",
    "$$\n",
    "\n",
    "For all the hidden units in a layer:\n",
    "\n",
    "```python\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[ 0.41492192  0.42604313  0.5002434 ]\n",
      "Output-layer Output:\n",
      "[ 0.49815196  0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_in_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_out = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_in_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_out)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dlnd]",
   "language": "python",
   "name": "conda-env-dlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
